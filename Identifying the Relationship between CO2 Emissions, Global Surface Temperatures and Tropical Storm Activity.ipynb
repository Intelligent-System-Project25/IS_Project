{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef8a598-e077-4e6b-a06d-1b9f99932765",
   "metadata": {},
   "source": [
    "Loading the dataset csv's into data frames and printing the first 10 values in the data set for each data frame to ensure that the data is being read and stored correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292e8cb-ab63-426b-b2f1-71bfdae3746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_1 = pd.read_csv(\"CO2 emission by countries.csv\")\n",
    "dataset_2 = pd.read_csv(\"Historical Tropical Storm.csv\")\n",
    "dataset_3 = pd.read_csv(\"global_temps.csv\")\n",
    "\n",
    "print(\"From Dataset 1: CO2 emission by countries\")\n",
    "print(dataset_1.head(10))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"From Dataset 2: Historical Tropical Storm\")\n",
    "print(dataset_2.head(10))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"From Dataset 3: global_temps\")\n",
    "print(dataset_3.head(10))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432aec3e",
   "metadata": {},
   "source": [
    "Cleaning the data by replacing nan values with approprate values for the specific column, by first identifying where the nan values are in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the number of missing values in the datasets\n",
    "print(\"Missing values from Dataset 1: CO2 emission by countries\")\n",
    "print(dataset_1.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Missing values from Dataset 2: Historical Tropical Storm\")\n",
    "print(dataset_2.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Missing values from Dataset 3: global_temps\")\n",
    "print(dataset_3.isnull().sum())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the nan values in the datasets for \"CO2 emission by countries\"\n",
    "\n",
    "#Identifying the categories in the \"Code\" column so I can match the format when filling in the nan values\n",
    "print(\"Unique Code Values\")\n",
    "print(dataset_1[\"Code\"].unique())\n",
    "print(\"n\")\n",
    "\n",
    "#Format 2 letters in all caps, if nan replace with \"XX\"\n",
    "dataset_1[\"Code\"] = dataset_1[\"Code\"].fillna(\"XX\") \n",
    "\n",
    "\n",
    "#Identifying the categories in the \"Calling Code\" column so I can match the format when filling in the nan values\n",
    "print(\"Unique Calling Code Values\")\n",
    "print(dataset_1[\"Calling Code\"].unique())\n",
    "print(\"n\")\n",
    "\n",
    "#Format 2 - 4 characters, if nan replace with \"000\"\n",
    "dataset_1[\"Calling Code\"] = dataset_1[\"Calling Code\"].fillna(\"000\") \n",
    "\n",
    "\n",
    "#Following the fomat I saw by overview the column, the values are int, I'll replace the missing population values with the avarage value of the population columns\n",
    "#Doing the same for the \"Area\" and \"% of World\"\n",
    "\n",
    "find_avg= [\"Population(2022)\", \"Area\"]#, \"% of World\"]\n",
    "\n",
    "#temp assigning the nan values to 0 for the summing\n",
    "dataset_1[find_avg] = dataset_1[find_avg].fillna(0)\n",
    "\n",
    "#casting to int\n",
    "dataset_1[find_avg] = dataset_1[find_avg].astype(int)\n",
    "\n",
    "agv = dataset_1[find_avg].mean()\n",
    "dataset_1[find_avg] = dataset_1[find_avg].fillna(agv) \n",
    "\n",
    "\n",
    "# \"% of World\" and \"Density(km2)\"\" seem to be of type string, replacing the nan values with \"Unknown\" <-- not feeling too sure about the method I used here\n",
    "nan_to_unknown= [\"% of World\", \"Density(km2)\"]\n",
    "dataset_1[nan_to_unknown] = dataset_1[nan_to_unknown].fillna(\"Unknown\")\n",
    "\n",
    "\n",
    "#Updating the dataset_1 csv\n",
    "dataset_1.to_csv(\"CO2 emission by countries.csv\", index= False)\n",
    "\n",
    "#Testing if the change was made\n",
    "print(\"Missing values from Dataset 1: CO2 emission by countries\")\n",
    "print(dataset_1.isnull().sum())\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b40032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the global_temps dataset\n",
    "\n",
    "\n",
    "#Filling in all nan values with the avarage value of their column\n",
    "find_avg_value= [\"Jun\",\"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\", \"J-D\", \"D-N\", \"DJF\", \"JJA\", \"SON\"]\n",
    "\n",
    "#temp assigning the nan values to 0 for the summing\n",
    "dataset_3[find_avg_value] = dataset_3[find_avg_value].fillna(0)\n",
    "\n",
    "#casting to float\n",
    "dataset_3[find_avg_value] = dataset_3[find_avg_value].astype(float)\n",
    "\n",
    "agv_value = dataset_3[find_avg_value].mean()\n",
    "dataset_3[find_avg_value] = dataset_3[find_avg_value].fillna(agv_value) \n",
    "\n",
    "\n",
    "#Updating the dataset_3 csv\n",
    "dataset_3.to_csv(\"global_temps.csv\", index= False)\n",
    "\n",
    "#Testing if the change was made\n",
    "print(\"Missing values from Dataset 3: global_temps\")\n",
    "print(dataset_3.isnull().sum())\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240668ac",
   "metadata": {},
   "source": [
    "Merging the 3 datasets together, joining dataset_1 \"CO2 emission by countries\" with dataset_2 \"Historical Tropical Storm\" using their years attribute, then taking at join dataset and connecting it to \"global_temps\" using the month and year attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d9e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changeing the attribute names to match where the are being joined\n",
    "dataset_2 = dataset_2.rename(columns={\"YEAR\": \"Year\"})\n",
    "\n",
    "#modifiy dataset_3 so it is more aligned with the format of the other data types for merging\n",
    "dataset_3 = pd.melt(dataset_3,\n",
    "                    id_vars=  [\"Year\"],\n",
    "                    value_vars=  [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"],\n",
    "                    var_name=  \"MONTH\", #matching dataset_2\n",
    "                    value_name=  \"Temp\")\n",
    "\n",
    "#dataset_2 has the months in numbers and dataset_3 has them in string, so I mapped the values\n",
    "mapping = {\"Jan\":1, \"Feb\":2, \"Mar\":3, \"Apr\":4, \"May\":5, \"Jun\":6, \"Jul\":7, \"Aug\":8, \"Sep\":9, \"Oct\":10, \"Nov\":11, \"Dec\":12}\n",
    "dataset_3[\"MONTH\"] = dataset_3[\"MONTH\"].map(mapping)\n",
    "\n",
    "dataset_4 = pd.merge(dataset_1, dataset_2, on= \"Year\", how= \"left\") #left for keeping all data from both csv for now\n",
    "\n",
    "#print(dataset_4.head(5))\n",
    "\n",
    "final_dataset = pd.merge(dataset_4, dataset_3, on= [\"Year\", \"MONTH\"], how= \"inner\") #inner for keeping only matching data from all csv\n",
    "\n",
    "#creating and writing to a new csv\n",
    "final_dataset.to_csv(\"completed_dataset_for_IS_project_25.csv\", index= False)\n",
    "\n",
    "#Loading the new csv\n",
    "final_dataset_values = pd.read_csv(\"completed_dataset_for_IS_project_25.csv\")\n",
    "\n",
    "#Printing the first 5 rows of the final dataset csv\n",
    "print(\"From the Final Dataset: completed_dataset_for_IS_project_25\")\n",
    "print(final_dataset_values.head(5))\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
