{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef8a598-e077-4e6b-a06d-1b9f99932765",
   "metadata": {},
   "source": [
    "Loading the dataset csv's into data frames and printing the first 10 values in the data set for each data frame to ensure that the data is being read and stored correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292e8cb-ab63-426b-b2f1-71bfdae3746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_1 = pd.read_csv(\"CO2 emission by countries.csv\")\n",
    "dataset_2 = pd.read_csv(\"Historical Tropical Storm.csv\")\n",
    "dataset_3 = pd.read_csv(\"global_temps.csv\")\n",
    "\n",
    "print(\"From Dataset 1: CO2 emission by countries\")\n",
    "print(dataset_1.head(10))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"From Dataset 2: Historical Tropical Storm\")\n",
    "print(dataset_2.head(10))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"From Dataset 3: global_temps\")\n",
    "print(dataset_3.head(10))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d0603",
   "metadata": {},
   "source": [
    "The \"CO2 emission by countries\" dataset currently has these attributes: Country\", \"Code\", \"Calling Code\", \"Year\", \"CO2 emission (Tons)\", \"Population(2022)\", \"Area\", \"% of World\" and \"Density(km2)\" but from this dataset we only require \"Year\" and \"CO2 emission (Tons)\" so the others will be removed to reduce the size of the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3959baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing unnecessary columns from the datasets \"CO2 emission by countries\"\n",
    "dataset_1.columns = dataset_1.columns.str.strip()\n",
    "dropping = [\"Country\", \"Code\", \"Calling Code\", \"Population(2022)\", \"Area\", \"% of World\", \"Density(km2)\"]\n",
    "dataset_1.drop(columns=dropping, inplace= True, errors= \"ignore\")\n",
    "\n",
    "#updating the csv\n",
    "dataset_1.to_csv(\"CO2 emission by countries.csv\", index= False)\n",
    "print(\"CO2 emission by countries dataset, has been updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a272e",
   "metadata": {},
   "source": [
    "The \"Historical Tropical Storm\" dataset currently has these attributes: \"FID\", \"YEAR\", \"MONTH\", \"DAY\", \"AD_TIME\", \"BTID\", \"NAME\", \"LAT\", \"LONG\", \"WIND_KTS\", \"PRESSURE\", \"CAT\", \"BASIN\" and \"Shape_Leng\" but from this dataset we only require \"YEAR\", \"MONTH\", \"DAY\", \"LAT\", \"LONG\", \"WIND_KTS\", \"PRESSURE\", \"CAT\" and \"Shape_Leng\" so the others will be removed to reduce the size of the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d8cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing unnecessary columns from the dataset \"Historical Tropical Storm\"\n",
    "dataset_2.columns = dataset_2.columns.str.strip()\n",
    "dropping_2 = [\"FID\", \"AD_TIME\", \"BTID\", \"NAME\", \"BASIN\"]\n",
    "dataset_2.drop(columns=dropping_2, inplace= True, errors= \"ignore\")\n",
    "\n",
    "#updating the csv\n",
    "dataset_2.to_csv(\"Historical Tropical Storm.csv\", index= False)\n",
    "print(\"Historical Tropical Storm dataset, has been updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4582ef54",
   "metadata": {},
   "source": [
    "The \"global_temps\" dataset currently has these attributes: \"Year\", \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\", \"J-D\", \"D-N\", \"DJF\", \"MAM\", \"JJA\" and \"SON\" but from this dataset we only require \"Year\", \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\"and \"Dec\" so the others will be removed to reduce the size of the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5949d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing unnecessary columns from the dataset \"global_temps\"\n",
    "dataset_3.columns = dataset_3.columns.str.strip()\n",
    "dropping_3 = [\"J-D\", \"D-N\", \"DJF\", \"MAM\", \"JJA\", \"SON\"]\n",
    "dataset_3.drop(columns=dropping_3, inplace= True, errors= \"ignore\")\n",
    "\n",
    "#updating the csv\n",
    "dataset_3.to_csv(\"global_temps.csv\", index= False)\n",
    "print(\"global_temps dataset, has been updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432aec3e",
   "metadata": {},
   "source": [
    "Cleaning the data by replacing nan values with approprate values for the specific column, by first identifying where the nan values are in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the number of missing values in the datasets\n",
    "print(\"Missing values from Dataset 1: CO2 emission by countries\")\n",
    "print(dataset_1.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Missing values from Dataset 2: Historical Tropical Storm\")\n",
    "print(dataset_2.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Missing values from Dataset 3: global_temps\")\n",
    "print(dataset_3.isnull().sum())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b40032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the global_temps dataset\n",
    "\n",
    "#Filling in all nan values with the avarage value of their column\n",
    "find_avg_value= [\"Jun\",\"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "\n",
    "#temp assigning the nan values to 0 for the summing\n",
    "dataset_3[find_avg_value] = dataset_3[find_avg_value].fillna(0)\n",
    "\n",
    "#casting to float\n",
    "dataset_3[find_avg_value] = dataset_3[find_avg_value].astype(float)\n",
    "\n",
    "agv_value = dataset_3[find_avg_value].mean()\n",
    "dataset_3[find_avg_value] = dataset_3[find_avg_value].fillna(agv_value) \n",
    "\n",
    "\n",
    "#Updating the dataset_3 csv\n",
    "dataset_3.to_csv(\"global_temps.csv\", index= False)\n",
    "\n",
    "#Testing if the change was made\n",
    "print(\"Missing values from Dataset 3: global_temps\")\n",
    "print(dataset_3.isnull().sum())\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06353350",
   "metadata": {},
   "source": [
    "Checking if the current data types for all attributes in all datasets are of the correct types for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788832fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_1\n",
    "\n",
    "#Before \n",
    "dataset_1.info()\n",
    "\n",
    "#The dataset is of the correct types, no chanages to be done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04baa383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_2\n",
    "\n",
    "#Before \n",
    "dataset_2.info()\n",
    "\n",
    "#The dataset is of the correct types, no chanages to be done here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbde84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_3\n",
    "\n",
    "dataset_3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240668ac",
   "metadata": {},
   "source": [
    "Merging the 3 datasets together, dataset_2 \"Historical Tropical Storm\" with joining dataset_1 \"CO2 emission by countries\" using their year attribute, then taking the joined datasets and connecting it to \"global_temps\" using the year attribute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d9e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#changing the attribute names to match where the are being joined\n",
    "dataset_2 = dataset_2.rename(columns={\"YEAR\": \"Year\"})\n",
    "\n",
    "dataset_4 = pd.merge(dataset_2, dataset_1, on= \"Year\", how= \"left\") #left for keeping all data from dataset_1\n",
    "\n",
    "print(\"Merged Dataset of Historical Tropical Storm and CO2 emission by countries\")\n",
    "print(dataset_4.head(5))\n",
    "\n",
    "final_dataset = pd.merge(dataset_4, dataset_3, on= \"Year\", how= \"left\") #left for keeping all data from dataset_4\n",
    "print(final_dataset.head(5))\n",
    "\n",
    "#creating and writing to a new csv\n",
    "\n",
    "with open(\"completed_dataset_for_IS_project_25.csv\", 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(final_dataset.columns)\n",
    "\n",
    "    for index, row in final_dataset.iterrows():\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Final Dataset csv has been created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e3ac7",
   "metadata": {},
   "source": [
    "The merged data set currently has 1,048,576 rows, I'm going to remove te rows that lacks the necessary information further reducing the size of the dataset\n",
    "\n",
    "The necessary columns are:  \"Year\", \"MONTH\", \"DAY\", \"LAT\", \"LONG\", \"WIND_KTS\", \"PRESSURE\", \"CAT\", \"Shape_Leng\", \"CO2 emission (Tons)\", \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\" and \"Dec\"\n",
    "\n",
    "If any data is missing then remove that row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a252274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "necessary_columns = [\"Year\", \"MONTH\", \"DAY\", \"LAT\", \"LONG\", \"WIND_KTS\", \"PRESSURE\", \"CAT\", \"Shape_Leng\", \"CO2 emission (Tons)\", \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "\n",
    "large_dataset = dd.read_csv(\"completed_dataset_for_IS_project_25.csv\")\n",
    "\n",
    "cleaned_final_dataset = large_dataset.dropna(subset=necessary_columns)\n",
    "\n",
    "print(cleaned_final_dataset.head(5))\n",
    "\n",
    "#Updating the dataset_3 csv\n",
    "cleaned_final_dataset.to_csv(\"completed_dataset_for_IS_project_25.csv\", index= False, single_file=True)\n",
    "\n",
    "print(\"The dataset is ready for processing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
